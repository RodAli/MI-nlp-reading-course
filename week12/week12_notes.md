## Language Models are Few-Shot Learners

#### OpenAI

Settings that we can evaluate GPT-3 on:

1. Fine-Turning (FT)
	- updating the weights of a pre-trained model by traning on a supervised dataset specific to a desired task
	- the main advantage is strong performance
	- disadvantage is generalizability
2. Few-Shot (FS)
	- where the model is given a few demonstrations of the task at inference time, but no weight updates are allowed
3. One-Shot (1S)
	- is the same as few-shot but with only one demonstration allowed
	- closely matches the way in which tasks are communicated to humans
4. Zero-Shot (0S)
	- is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task

Datasets used to train GPT-3:

1. Common Crawl - 410 billion tokens
2. WebText2 - 19 billion tokens
3. Books1 - 12 billion tokens
4. Books2 - 55 billion tokens
5. Wikipedia - 3 billion tokens

## The messy, secretive reality behind OpenAIâ€™s bid to save the world

#### By Karen Hao

Link: https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/


## Language Models and Fake News: the Democratization of Propaganda

#### Adrian Yijie Xu

Link: https://towardsdatascience.com/language-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054

